# BOF_ Vulnerability Data Consumers

**Video URL:** [https://www.youtube.com/watch?v=GyFyXvmlxHY](https://www.youtube.com/watch?v=GyFyXvmlxHY)
**Video ID:** GyFyXvmlxHY

---

SUMMARY
Art O'Keefe discusses vulnerability data challenges, including data format, accuracy, updates, and consumer feedback with security researchers.

IDEAS
* Parsing XML versus JSON data formats presents challenges for vulnerability data consumption.
* Normalizing vulnerability data across different sources is a complex and costly process.
* Keeping up with vulnerability data updates and prioritizing them effectively is difficult.
* Relying on threat feeds and vendor updates is a common solution for tracking active exploits.
* Data accuracy and shifts in format over time raise concerns about the reliability of CVE data.
* Expert interpretation is often needed to understand the meaning of multiple configurations in CVEs.
* The switch from version ranges to specified versions has introduced inaccuracies in CVE data.
* Data quality issues, including free text descriptions and missing data, are significant challenges.
* The 80/20 rule for data scientists applies: 80% cleaning, 20% complaining about data.
* Enriching imperfect data after the fact is a common workaround for data quality issues.
* Balancing strictness in CNA rules with encouraging vulnerability submissions is crucial.
* Escalation of vulnerability severity over time requires constant monitoring and updates.
* Querying and comparing vulnerabilities manually is time-consuming and inefficient.
* MITER has updated the schema for NVD CPE format to address some data quality issues.
* Consumer feedback and a "consumer union" could help improve CVE data quality.
* The CVE Quality Working Group's GitHub repo provides a platform for feedback and issue reporting.
* NVD's change log is difficult to use, making it hard to track changes effectively.
* Resolving GitHub diffs into JSON files is a complex process for tracking CVE changes.
* Flattening nested array data is a common task for analysis and visualization.
* A flat file pipeline for CVE data could simplify analysis and visualization.
* Mapping vulnerabilities to threat intelligence is the next evolution of vulnerability management.
* Supporting and improving the CVE process, despite its flaws, is essential for long-term progress.
* Tracking CV IDs is crucial for understanding and responding to attacker activity.
* Bespoke processes for normalizing and flattening CVE data lead to duplication of effort.
* An event stream model could improve data consumption and reduce duplication of effort.
* Prioritizing signal over noise in vulnerability data is a key challenge for consumers.
* The high volume and frequency of updates pose challenges for data consumption.
* The CVE program needs to be viewed as a product with both direct and downstream customers.
* Product managers should be obsessive about understanding customer problems and needs.
* Regular customer interviews are crucial for understanding and addressing real-world challenges.
* The challenges faced by CISOs may differ from those discussed within the CVE community.
* Continuing the dialogue and gathering feedback from diverse stakeholders is essential.

INSIGHTS
* Data quality and consistency are major obstacles to effective vulnerability management.
* Balancing data completeness with ease of submission is key to a successful CVE program.
* Automation and standardization are needed to improve data processing and reduce manual effort.
* Consumer feedback and engagement are crucial for driving improvements in CVE data quality.
* The CVE program should adopt a product management approach focused on customer needs.
* Threat intelligence integration is essential for understanding the real-world impact of vulnerabilities.
* The current CVE data model hinders efficient consumption and analysis.
* Collaboration and information sharing are key to addressing the challenges of vulnerability data.
* A shift towards an event-driven model could revolutionize vulnerability data consumption.
* Focusing on the needs of downstream customers will lead to more effective vulnerability management.

QUOTES
* "A lot of the CVE data used to be in XML and now it's in JSON which was in my opinion a wonderful change." - Art O'Keefe
* "CVE descriptions are often pathetic. It's if not coming from the big vendors." - Discord User
* "80% of your time cleaning data and the other 20% complaining about the data." - Art O'Keefe
* "So we'll take a lot of these the data in from I guess CVE and it's it's good." - Audience Member
* "There's 11 billion CVEes around there and some of them are old and we find new stuff too." - Audience Member
* "How do we use it to prioritize?" - Audience Member
* "There are too many vulnerabilities for us to patch." - Lord
* "Really good product managers, they become obsessive with the customer." - Lord
* "The things they told me they didn't like about the CV program are not the things we've been arguing about." - Lord

HABITS
* Error checking for missing fields and unexpected characters in vulnerability data.
* Leaving error checking in place to catch recurring data quality issues.
* Writing one-off scripts to handle specific data inconsistencies, like "N/A" values.
* Relying on threat feeds from vendors like Cisco and Microsoft for updates.
* Periodically checking vendor releases for updates and concerning vulnerabilities.
* Manually comparing vulnerability data to identify changes and updates.
* Using bash and jq for data processing and analysis.
* Flattening nested array data for analysis and visualization.
* Opening GitHub issues to provide feedback and suggest improvements.
* Connecting with others on LinkedIn to discuss vulnerability data challenges.

FACTS
* CVE data is available in both XML and JSON formats.
* There are approximately 11 billion CVEs.
* EPSS scores vulnerabilities daily.
* MITER has updated the schema for NVD CPE format.
* The CVE Quality Working Group has a GitHub repository.
* NVD has a change log, but it's difficult to use.
* The CVE list is available on GitHub.
* Disk space is relatively inexpensive.
* There are too many vulnerabilities to patch them all.

REFERENCES
* CVE.org
* NVD
* OSV (GitHub Advisories)
* JPER
* CNNVD
* Splunk
* CVSS
* CWE
* EPSS
* CVE Quality Working Group GitHub Repo
* Secure by Design Program at Cisco
* CISA

ONE-SENTENCE TAKEAWAY
Collaborate and improve CVE data quality through consumer feedback and standardized processes.

RECOMMENDATIONS
* Standardize data formats and improve data quality for easier consumption and analysis.
* Implement an event stream model for real-time updates and reduced duplication.
* Create a centralized platform for consumer feedback and collaboration on CVE improvements.
* Develop tools and processes for efficient normalization and flattening of CVE data.
* Prioritize vulnerabilities based on threat intelligence and active exploit information.
* Foster communication between CVE providers, tool vendors, and end-users.
* Encourage participation in the CVE Quality Working Group and provide constructive feedback.
* Leverage existing resources like the GitHub repo and discussions for collaboration.
* Focus on the needs of downstream customers and prioritize their challenges.
* View the CVE program as a product and adopt a customer-centric approach.
* Conduct regular customer interviews to understand real-world needs and pain points.
* Support and improve the CVE process, even if it's currently flawed, for long-term benefit.
* Track CV IDs to understand attacker activity and prioritize mitigation efforts.
* Share bespoke processes and tools to reduce duplication of effort and foster collaboration.
* Consider the signal-to-noise ratio and develop strategies for efficient data filtering.
