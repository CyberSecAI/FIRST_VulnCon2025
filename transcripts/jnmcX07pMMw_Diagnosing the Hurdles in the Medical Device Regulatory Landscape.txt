Kind: captions Language: en Everybody will be presenting on Everybody will be presenting on Everybody will be presenting on diagnosing the hurdles of the medical diagnosing the hurdles of the medical diagnosing the hurdles of the medical regulatory landscape. We'll be mostly regulatory landscape. We'll be mostly regulatory landscape. We'll be mostly connected with US and their connected with US and their connected with US and their regulatory regulatory regulatory guidelines. Just to go over what is anac guidelines. Just to go over what is anac guidelines. Just to go over what is anac is an information sharing analysis is an information sharing analysis is an information sharing analysis center. Um so we empower sharing center. Um so we empower sharing center. Um so we empower sharing collaboration in critical infrastructure collaboration in critical infrastructure collaboration in critical infrastructure and so you have there's and so you have there's and so you have there's different we are the health different we are the health different we are the health sector water you guys so health is a community of 10,000 guys so health is a community of 10,000 guys so health is a community of 10,000 plus security analysts more trust plus security analysts more trust plus security analysts more trust uh we focus on different areas not only uh we focus on different areas not only uh we focus on different areas not only the healthcare providers but the medical the healthcare providers but the medical the healthcare providers but the medical device manufacturers we're even going device manufacturers we're even going device manufacturers we're even going into and so this talk it was diagnosing the and so this talk it was diagnosing the and so this talk it was diagnosing the hurdles in regulatory connect the hurdles in regulatory connect the hurdles in regulatory connect the regulatory landscape and a lot of those regulatory landscape and a lot of those regulatory landscape and a lot of those hurdles uh this talk is going to focus hurdles uh this talk is going to focus hurdles uh this talk is going to focus on AI immigration and how AI immigration on AI immigration and how AI immigration on AI immigration and how AI immigration standards kind of making the process of standards kind of making the process of standards kind of making the process of getting medical devices approved or just getting medical devices approved or just getting medical devices approved or just R&amp;D in general across the board and R&amp;D in general across the board and R&amp;D in general across the board and global future a bit more complicated. So global future a bit more complicated. So global future a bit more complicated. So to start it off, AI is used in medical to start it off, AI is used in medical to start it off, AI is used in medical devices right now to analyze devices right now to analyze devices right now to analyze medical stream diagnosis and kind of medical stream diagnosis and kind of medical stream diagnosis and kind of prepare a little bit quicker. Uh it's prepare a little bit quicker. Uh it's prepare a little bit quicker. Uh it's really useful in monitoring wearable really useful in monitoring wearable really useful in monitoring wearable stuff like your Fitbits or Apple Watch stuff like your Fitbits or Apple Watch stuff like your Fitbits or Apple Watch devices. AI kind of that data into more devices. AI kind of that data into more devices. AI kind of that data into more actual insights. And then arguably one actual insights. And then arguably one actual insights. And then arguably one of the of the of the first use cases after with LLM in first use cases after with LLM in first use cases after with LLM in healthcare delivery environments healthcare delivery environments healthcare delivery environments hospital notes hospital notes hospital notes during patient position. So that during patient position. So that during patient position. So that way you can refer them way you can refer them way you can refer them to. So the risk of AI medical devices to. So the risk of AI medical devices to. So the risk of AI medical devices like every new disruptive technology like every new disruptive technology like every new disruptive technology there some risk that implementing it. So there some risk that implementing it. So there some risk that implementing it. So for AI specifically in medical devices for AI specifically in medical devices for AI specifically in medical devices data quality bias so the AI is not data quality bias so the AI is not data quality bias so the AI is not biased it has the bias of whatever biased it has the bias of whatever biased it has the bias of whatever training data is because it's a training data is because it's a training data is because it's a mathematical model at the end of the day mathematical model at the end of the day mathematical model at the end of the day it will reflect that same bias that has it will reflect that same bias that has it will reflect that same bias that has in training data set regulatory in training data set regulatory in training data set regulatory challenges which more deeper in it challenges which more deeper in it challenges which more deeper in it presentation of kind of how different presentation of kind of how different presentation of kind of how different regions around the world standardized AI regions around the world standardized AI regions around the world standardized AI use and how device manufacturing groups use and how device manufacturing groups use and how device manufacturing groups were to navigate different standards of were to navigate different standards of were to navigate different standards of use and become and maintain medical use and become and maintain medical use and become and maintain medical presence cyber security of medical presence cyber security of medical presence cyber security of medical devices with AI data data loss devices with AI data data loss devices with AI data data loss preventions and DID concerns the data preventions and DID concerns the data preventions and DID concerns the data implemented into the AI is not secured implemented into the AI is not secured implemented into the AI is not secured properly while protected health properly while protected health properly while protected health information privacy concerns so by information privacy concerns so by information privacy concerns so by integrating AI we're also opening up the integrating AI we're also opening up the integrating AI we're also opening up the space which health information is being space which health information is being space which health information is being transferred therefore transferred therefore transferred therefore your translication So as we see later on some blocks of So as we see later on some blocks of So as we see later on some blocks of countries around the world have countries around the world have countries around the world have these standards for AI use and because these standards for AI use and because these standards for AI use and because health information was so sensitive health information was so sensitive health information was so sensitive ethical concern play a major part in ethical concern play a major part in ethical concern play a major part in development. So moving on um this is a development. So moving on um this is a development. So moving on um this is a hypothetical attack. It has to deal with hypothetical attack. It has to deal with hypothetical attack. It has to deal with poisoning of images. So kind of poisoning of images. So kind of poisoning of images. So kind of introduce us a little bit where AI is introduce us a little bit where AI is introduce us a little bit where AI is mclassification. So these models that mclassification. So these models that mclassification. So these models that will identify what an image is they can will identify what an image is they can will identify what an image is they can be poisoned. So this is an example of a be poisoned. So this is an example of a be poisoned. So this is an example of a model that degradation attack because model that degradation attack because model that degradation attack because you're creating the effectiveness of the you're creating the effectiveness of the you're creating the effectiveness of the modeling distorted pixels which are modeling distorted pixels which are modeling distorted pixels which are called noise images. That plus 0.07x 07x called noise images. That plus 0.07x 07x called noise images. That plus 0.07x 07x is actually a mathematical function done is actually a mathematical function done is actually a mathematical function done with the original panda and some of the with the original panda and some of the with the original panda and some of the research that have come out universities research that have come out universities research that have come out universities but they've actually been able to create images. So this one was given and then images. So this one was given and then images. So this one was given and then the bottom the bottom the bottom one so you can kind of get these one so you can kind of get these one so you can kind of get these predetermined ratio predetermined ratio predetermined ratio pixels. So what does that mean for pixels. So what does that mean for pixels. So what does that mean for healthare? Um basically with medical healthare? Um basically with medical healthare? Um basically with medical imaging you'd be given these imaging imaging you'd be given these imaging imaging you'd be given these imaging devices for a diagnosis. Hypothetically devices for a diagnosis. Hypothetically devices for a diagnosis. Hypothetically if these devices comes into that data if these devices comes into that data if these devices comes into that data bias we were talking about earlier. If a bias we were talking about earlier. If a bias we were talking about earlier. If a threat actor were to poison the training threat actor were to poison the training threat actor were to poison the training data of AI with these noise images um data of AI with these noise images um data of AI with these noise images um stuff like this would occur which really stuff like this would occur which really stuff like this would occur which really implements the need for human to pre because this is an example of AI pre because this is an example of AI pre because this is an example of AI hallucination which is where the AI's hallucination which is where the AI's hallucination which is where the AI's result is factually incorrect and this result is factually incorrect and this result is factually incorrect and this has been a persistent problem even newer has been a persistent problem even newer has been a persistent problem even newer models by ch by Google by Microsoft models by ch by Google by Microsoft models by ch by Google by Microsoft Sure. Sure. Sure. So this is a new this is a problem So this is a new this is a problem So this is a new this is a problem that's still persisting in some of the that's still persisting in some of the that's still persisting in some of the new models like uh open AI's 4.5 or new models like uh open AI's 4.5 or new models like uh open AI's 4.5 or Google's GMA free their open source Google's GMA free their open source Google's GMA free their open source series of models uh no matter how series of models uh no matter how series of models uh no matter how advanced it gets these problems persist advanced it gets these problems persist advanced it gets these problems persist so the need for humans to validate trust so the need for humans to validate trust so the need for humans to validate trust but verify that's the same uh definitely but verify that's the same uh definitely but verify that's the same uh definitely rings true with AI then especially when rings true with AI then especially when rings true with AI then especially when you're adopting it into devices you're adopting it into devices you're adopting it into devices especially medical devices that deal especially medical devices that deal especially medical devices that deal with sensitive information that human with sensitive information that human with sensitive information that human element is still very much needed. So AI element is still very much needed. So AI element is still very much needed. So AI does not replace humans. It just kind of does not replace humans. It just kind of does not replace humans. It just kind of acts as a tool to empower them. So for the legislation on AI and medical So for the legislation on AI and medical So for the legislation on AI and medical devices, as I said, we'll be focusing on devices, as I said, we'll be focusing on devices, as I said, we'll be focusing on the United States and as well as EU. for the United States and as well as EU. for the United States and as well as EU. for the European Union um they do have some the European Union um they do have some the European Union um they do have some AI apps and then for specific countries AI apps and then for specific countries AI apps and then for specific countries by themselves like Spain has really led by themselves like Spain has really led by themselves like Spain has really led the medical device security landscape the medical device security landscape the medical device security landscape with their own with their own with their own regulations. So for the United States um regulations. So for the United States um regulations. So for the United States um for medical devices specifically we're for medical devices specifically we're for medical devices specifically we're going to be looking at primarily the going to be looking at primarily the going to be looking at primarily the premarket submissions as well as a 510k premarket submissions as well as a 510k premarket submissions as well as a 510k which will allow a device to be marketed which will allow a device to be marketed which will allow a device to be marketed on in the market. Um, so the FDA has on in the market. Um, so the FDA has on in the market. Um, so the FDA has been authorizing more AIS every year been authorizing more AIS every year been authorizing more AIS every year since 1995 to 2024. There's been 950 AI since 1995 to 2024. There's been 950 AI since 1995 to 2024. There's been 950 AI in medical devices approved. The chart in medical devices approved. The chart in medical devices approved. The chart just found that out because the data is just found that out because the data is just found that out because the data is not complete, but it is going to not complete, but it is going to not complete, but it is going to continue to rise. continue to rise. continue to rise. Um, so for the pre-market submissions, Um, so for the pre-market submissions, Um, so for the pre-market submissions, there's a lot of draft guides and there's a lot of draft guides and there's a lot of draft guides and guidances that the FDA has released to guidances that the FDA has released to guidances that the FDA has released to help manufacturers get those devices help manufacturers get those devices help manufacturers get those devices approved. One of those is different approved. One of those is different approved. One of those is different considerations for use of artificial considerations for use of artificial considerations for use of artificial intelligence to support regulatory intelligence to support regulatory intelligence to support regulatory decision making for drug biological decision making for drug biological decision making for drug biological products. The main thing with um medical products. The main thing with um medical products. The main thing with um medical devices is that patient safety aspect. devices is that patient safety aspect. devices is that patient safety aspect. So um when looking at the cyber security So um when looking at the cyber security So um when looking at the cyber security of a device we have to consider patient of a device we have to consider patient of a device we have to consider patient safety as rather than just the safety as rather than just the safety as rather than just the vulnerability we have to think that vulnerability we have to think that vulnerability we have to think that patient safety aspect there's also the patient safety aspect there's also the patient safety aspect there's also the AI and um ML software as a mode action AI and um ML software as a mode action AI and um ML software as a mode action plan they has released as well as the plan they has released as well as the plan they has released as well as the draft guidance on artificial draft guidance on artificial draft guidance on artificial intelligence enabled device software intelligence enabled device software intelligence enabled device software functions life cycle management and functions life cycle management and functions life cycle management and marketing submission this is going to be marketing submission this is going to be marketing submission this is going to be from your pre-market to from your from your pre-market to from your from your pre-market to from your development to your upload like it's development to your upload like it's development to your upload like it's going to complete those recommendations going to complete those recommendations going to complete those recommendations um as well as a risk management um as well as a risk management um as well as a risk management um um um document and then for other regulations document and then for other regulations document and then for other regulations for the United States. Previously the for the United States. Previously the for the United States. Previously the Biden administration had implemented an Biden administration had implemented an Biden administration had implemented an executive executive executive order order order one zero regarding the uses of AI that one zero regarding the uses of AI that one zero regarding the uses of AI that was mostly the federal agencies had to was mostly the federal agencies had to was mostly the federal agencies had to maintain or the use of AI. However, the maintain or the use of AI. However, the maintain or the use of AI. However, the executive order recently by repelled executive order recently by repelled executive order recently by repelled that executive order. So able to that executive order. So able to that executive order. So able to consider that forum um as well as just consider that forum um as well as just consider that forum um as well as just managing risk and considerations for the managing risk and considerations for the managing risk and considerations for the FDA that drop as well as the post market updates. So the European Union updates. So the European Union updates. So the European Union introduced a first on the local stage introduced a first on the local stage introduced a first on the local stage where they were the first organization where they were the first organization where they were the first organization to release a comprehensive use case of to release a comprehensive use case of to release a comprehensive use case of what was an acceptable use of AI. They what was an acceptable use of AI. They what was an acceptable use of AI. They classified the uses of AI into four classified the uses of AI into four classified the uses of AI into four categories. They had unacceptable risk, categories. They had unacceptable risk, categories. They had unacceptable risk, high risk, um unacceptable risk, high risk, limited um unacceptable risk, high risk, limited um unacceptable risk, high risk, limited risk and minimum risk. So these were the risk and minimum risk. So these were the risk and minimum risk. So these were the four categories that AI use and this four categories that AI use and this four categories that AI use and this affect not only medical devices as they affect not only medical devices as they affect not only medical devices as they are high risk applications but just are high risk applications but just are high risk applications but just generally um all other innovators that generally um all other innovators that generally um all other innovators that seem to do business in the European seem to do business in the European seem to do business in the European Union. And this was pretty monumental Union. And this was pretty monumental Union. And this was pretty monumental because uh regardless of where the because uh regardless of where the because uh regardless of where the provider or the developer the AI system provider or the developer the AI system provider or the developer the AI system was located, they had to adhere to the was located, they had to adhere to the was located, they had to adhere to the development standards that correspond to development standards that correspond to development standards that correspond to the category of their product. So if for the category of their product. So if for the category of their product. So if for example at healthc we have a lot of example at healthc we have a lot of example at healthc we have a lot of medical device manufacturers and medical device manufacturers and medical device manufacturers and healthcare delivery organizations and healthcare delivery organizations and healthcare delivery organizations and the AI boosted products in the the AI boosted products in the the AI boosted products in the healthcare sector are considered high healthcare sector are considered high healthcare sector are considered high risk. So there's a lot of regulatory risk. So there's a lot of regulatory risk. So there's a lot of regulatory changes that we're going to need to changes that we're going to need to changes that we're going to need to accommodate. And as Taylor was accommodate. And as Taylor was accommodate. And as Taylor was mentioning earlier with executive orders mentioning earlier with executive orders mentioning earlier with executive orders in AI, the there was a summit I think in in AI, the there was a summit I think in in AI, the there was a summit I think in Paris where a lot of global leaders met Paris where a lot of global leaders met Paris where a lot of global leaders met to discuss what the development of AI to discuss what the development of AI to discuss what the development of AI meant in their countries. And there was meant in their countries. And there was meant in their countries. And there was two schools of thought during that two schools of thought during that two schools of thought during that conference that kind of shown through. conference that kind of shown through. conference that kind of shown through. There was the um making sure we secure There was the um making sure we secure There was the um making sure we secure and develop these ethical frameworks and develop these ethical frameworks and develop these ethical frameworks before we develop AI and then there was before we develop AI and then there was before we develop AI and then there was the innovation the innovation the innovation um where we just innovate that we can um where we just innovate that we can um where we just innovate that we can regulate after the fact to maintain a regulate after the fact to maintain a regulate after the fact to maintain a competitive edge. So the reasoning for competitive edge. So the reasoning for competitive edge. So the reasoning for transitioning from those executive transitioning from those executive transitioning from those executive orders off of United States companies orders off of United States companies orders off of United States companies where United States based companies do where United States based companies do where United States based companies do not have to adhere to not have to adhere to not have to adhere to um ethical use as much as EU companies um ethical use as much as EU companies um ethical use as much as EU companies do because the European Union is kind of do because the European Union is kind of do because the European Union is kind of like in this ethical use of AI. So, it's like in this ethical use of AI. So, it's like in this ethical use of AI. So, it's going to be it's going to create quite going to be it's going to create quite going to be it's going to create quite the dynamic for organizations that do the dynamic for organizations that do the dynamic for organizations that do business in the US and the EU and abroad business in the US and the EU and abroad business in the US and the EU and abroad because they're going to be two because they're going to be two because they're going to be two different mentalities regarding different mentalities regarding different mentalities regarding innovation at play. you're going to have innovation at play. you're going to have innovation at play. you're going to have to somehow navigate the ethical use that to somehow navigate the ethical use that to somehow navigate the ethical use that is persistent prioritized in the is persistent prioritized in the is persistent prioritized in the European Union and the innovative use European Union and the innovative use European Union and the innovative use case that is prioritized in the case that is prioritized in the case that is prioritized in the US and because the medical device are US and because the medical device are US and because the medical device are classified as high risk um they have to classified as high risk um they have to classified as high risk um they have to undergo conformity assessments for undergo conformity assessments for undergo conformity assessments for compliance and there's a lot of supply compliance and there's a lot of supply compliance and there's a lot of supply chain marketing involved in this chain marketing involved in this chain marketing involved in this process. So here are some obligations process. So here are some obligations process. So here are some obligations that are required for high-risk systems. that are required for high-risk systems. that are required for high-risk systems. Um they need to have technical Um they need to have technical Um they need to have technical documentation. So documentation of how documentation. So documentation of how documentation. So documentation of how your system works exactly. Transparency your system works exactly. Transparency your system works exactly. Transparency and provision of information to and provision of information to and provision of information to developers uh sorry to employers. This developers uh sorry to employers. This developers uh sorry to employers. This talks about kind of these sbombs is the talks about kind of these sbombs is the talks about kind of these sbombs is the transparency of what went into your transparency of what went into your transparency of what went into your device. Quality management system making device. Quality management system making device. Quality management system making sure you got good QA because it's going sure you got good QA because it's going sure you got good QA because it's going to be used in a high risk application. to be used in a high risk application. to be used in a high risk application. Um, making sure you that good QA is Um, making sure you that good QA is Um, making sure you that good QA is definitely a high priority. Having an definitely a high priority. Having an definitely a high priority. Having an incident reporting apparatus readily incident reporting apparatus readily incident reporting apparatus readily available so if something does happen, available so if something does happen, available so if something does happen, it will be remediated. In conformity it will be remediated. In conformity it will be remediated. In conformity assessments, this is going to make sure assessments, this is going to make sure assessments, this is going to make sure that your development life cycle and that your development life cycle and that your development life cycle and your system itself adhere to the your system itself adhere to the your system itself adhere to the regulations at play. Postmarketing regulations at play. Postmarketing regulations at play. Postmarketing monitoring procedures to make sure that monitoring procedures to make sure that monitoring procedures to make sure that they maintain compliance. Then AI they maintain compliance. Then AI they maintain compliance. Then AI literacy, making sure your company has a literacy, making sure your company has a literacy, making sure your company has a baseline in ethical use of AI systems. baseline in ethical use of AI systems. baseline in ethical use of AI systems. So some standardization challenges. Um So some standardization challenges. Um So some standardization challenges. Um here we have the pyramid where we have here we have the pyramid where we have here we have the pyramid where we have our four tiers of AI use. We have the our four tiers of AI use. We have the our four tiers of AI use. We have the unacceptable use or sorry unacceptable unacceptable use or sorry unacceptable unacceptable use or sorry unacceptable risk which is prohibited. These include risk which is prohibited. These include risk which is prohibited. These include um some facial recognition. Uh these AI um some facial recognition. Uh these AI um some facial recognition. Uh these AI large scale social engineering. So like large scale social engineering. So like large scale social engineering. So like a pretty common use case of this would a pretty common use case of this would a pretty common use case of this would be how they have a social credit system be how they have a social credit system be how they have a social credit system in China and other countries around the in China and other countries around the in China and other countries around the world. We're using AI for involuntary world. We're using AI for involuntary world. We're using AI for involuntary facial recognition. Uh that would be facial recognition. Uh that would be facial recognition. Uh that would be considered unacceptable risk of AI and considered unacceptable risk of AI and considered unacceptable risk of AI and therefore prohibited use case the high therefore prohibited use case the high therefore prohibited use case the high risk where the health care system the risk where the health care system the risk where the health care system the health sector operates. Uh this is going health sector operates. Uh this is going health sector operates. Uh this is going to where you need to have thatformity to where you need to have thatformity to where you need to have thatformity assessment. This is where you're going assessment. This is where you're going assessment. This is where you're going to need to have your risk management to need to have your risk management to need to have your risk management some of that heightened transparency uh some of that heightened transparency uh some of that heightened transparency uh data governance and then lastly of the data governance and then lastly of the data governance and then lastly of the site. site. site. So in dealing with these high-risk So in dealing with these high-risk So in dealing with these high-risk requirements while also maintaining a requirements while also maintaining a requirements while also maintaining a competitive edge as this EU AI act competitive edge as this EU AI act competitive edge as this EU AI act becomes enforced this could lead to becomes enforced this could lead to becomes enforced this could lead to bottlenecks in development as you're bottlenecks in development as you're bottlenecks in development as you're trying to push out products that push trying to push out products that push trying to push out products that push the curve while also maintaining to the curve while also maintaining to the curve while also maintaining to these new ethical standards. Whereas these new ethical standards. Whereas these new ethical standards. Whereas maybe competitors that just have a maybe competitors that just have a maybe competitors that just have a US-based company won't have to go to US-based company won't have to go to US-based company won't have to go to that same regulatory oversight. they that same regulatory oversight. they that same regulatory oversight. they will have a smaller market but their will have a smaller market but their will have a smaller market but their innovation curve may be slightly steeper innovation curve may be slightly steeper innovation curve may be slightly steeper because they don't have to engage in the because they don't have to engage in the because they don't have to engage in the compliance measures that would lead to compliance measures that would lead to compliance measures that would lead to some bottlenecks in development. So R&amp;D some bottlenecks in development. So R&amp;D some bottlenecks in development. So R&amp;D could slow down once until that could slow down once until that could slow down once until that framework is established. Um and then so some impacts established. Um and then so some impacts established. Um and then so some impacts as medical device manufacturers and as medical device manufacturers and as medical device manufacturers and other device manufacturers and honestly other device manufacturers and honestly other device manufacturers and honestly many innovators really um ensuring local many innovators really um ensuring local many innovators really um ensuring local compliance is going to take some compliance is going to take some compliance is going to take some significant amount of overhead. You're significant amount of overhead. You're significant amount of overhead. You're going to have to probably allocate some going to have to probably allocate some going to have to probably allocate some resources to make sure that your R&amp;D resources to make sure that your R&amp;D resources to make sure that your R&amp;D centers are aware of these legislations centers are aware of these legislations centers are aware of these legislations and are also aware of the building and are also aware of the building and are also aware of the building infrastructure to make sure that you are infrastructure to make sure that you are infrastructure to make sure that you are compliant as these legislations are the compliant as these legislations are the compliant as these legislations are the UAI act while it's a great baseline. Um UAI act while it's a great baseline. Um UAI act while it's a great baseline. Um it is likely that it's going to be built it is likely that it's going to be built it is likely that it's going to be built upon and there's also legislation upon and there's also legislation upon and there's also legislation similar to it uh in the Asia Pacific similar to it uh in the Asia Pacific similar to it uh in the Asia Pacific region longer lead time. So this is more region longer lead time. So this is more region longer lead time. So this is more of that when you're diverting resources of that when you're diverting resources of that when you're diverting resources to making sure to maintain compliance to making sure to maintain compliance to making sure to maintain compliance may lead to a may lead to a may lead to a decrease in operational output decrease in operational output decrease in operational output especially on the R&amp;D side of things especially on the R&amp;D side of things especially on the R&amp;D side of things where you're trying to come up with new where you're trying to come up with new where you're trying to come up with new products some initiatives may be products some initiatives may be products some initiatives may be soliance increase that's resource soliance increase that's resource soliance increase that's resource allocation we're talking about um allocation we're talking about um allocation we're talking about um establishing regulatory teams and then establishing regulatory teams and then establishing regulatory teams and then some compliance regulations may overlap some compliance regulations may overlap some compliance regulations may overlap this is actually a good thing because if this is actually a good thing because if this is actually a good thing because if multiple multiple multiple global compliance regulations that global compliance regulations that global compliance regulations that emerged out of this CUI act with similar emerged out of this CUI act with similar emerged out of this CUI act with similar acts to follow. If you can find a way to acts to follow. If you can find a way to acts to follow. If you can find a way to navigate the same problem that's navigate the same problem that's navigate the same problem that's persistent across multiple um countries persistent across multiple um countries persistent across multiple um countries and making sure you have transparency, and making sure you have transparency, and making sure you have transparency, making sure you have that human making sure you have that human making sure you have that human oversight element, figuring out common oversight element, figuring out common oversight element, figuring out common denominators can be a way to streamline and specifically for the United States and specifically for the United States and specifically for the United States manufacturers will have to comply with manufacturers will have to comply with manufacturers will have to comply with both I think the security rule which is both I think the security rule which is both I think the security rule which is the 524B and I'll include also what the the 524B and I'll include also what the the 524B and I'll include also what the proposed type of security edition And proposed type of security edition And proposed type of security edition And they will also have to comply with that they will also have to comply with that they will also have to comply with that which will include timelines for which will include timelines for which will include timelines for patching timelines for risk management patching timelines for risk management patching timelines for risk management um which will have to occur on a more um which will have to occur on a more um which will have to occur on a more regular basis between six months to a regular basis between six months to a regular basis between six months to a year rather than whenever you come out year rather than whenever you come out year rather than whenever you come out with a patch and there will also be FDA with a patch and there will also be FDA with a patch and there will also be FDA oversight once again with that premarket oversight once again with that premarket oversight once again with that premarket submission the 5 to 10k and then the submission the 5 to 10k and then the submission the 5 to 10k and then the guidance documents um available in a guidance documents um available in a guidance documents um available in a submissions which will help get those submissions which will help get those submissions which will help get those devices over Then for companies operating in you must Then for companies operating in you must Then for companies operating in you must comply with medical device regulation. comply with medical device regulation. comply with medical device regulation. Um additional data privacy regulations Um additional data privacy regulations Um additional data privacy regulations subject to subject to subject to GDPR extra requirements like having data GDPR extra requirements like having data GDPR extra requirements like having data guarding similar goals. Also EU guarding similar goals. Also EU guarding similar goals. Also EU companies will have to comply with the companies will have to comply with the companies will have to comply with the EUAI act which we covered earlier. uh EUAI act which we covered earlier. uh EUAI act which we covered earlier. uh really some of that strict compliance really some of that strict compliance really some of that strict compliance with that risk management and data with that risk management and data with that risk management and data governance and also you are required to governance and also you are required to governance and also you are required to set up reporting systems and even if set up reporting systems and even if set up reporting systems and even if you're not an EU specific company if you you're not an EU specific company if you you're not an EU specific company if you are trying to do business in the are trying to do business in the are trying to do business in the European Union or you're trying to European Union or you're trying to European Union or you're trying to expand globally expand globally expand globally um proactive measures to try to come up um proactive measures to try to come up um proactive measures to try to come up with those frameworks compliance might with those frameworks compliance might with those frameworks compliance might be a good way to streamline that be a good way to streamline that be a good way to streamline that process. And overall just globally you're going And overall just globally you're going And overall just globally you're going to have to conform to different regions to have to conform to different regions to have to conform to different regions and AI regulations. So currently there and AI regulations. So currently there and AI regulations. So currently there is regulations with the United States, is regulations with the United States, is regulations with the United States, EU, UK, Singapore, China, Brazil just to EU, UK, Singapore, China, Brazil just to EU, UK, Singapore, China, Brazil just to name a few or Australia specifically name a few or Australia specifically name a few or Australia specifically which is an obvious list but the which is an obvious list but the which is an obvious list but the therapeutic goods administration is um therapeutic goods administration is um therapeutic goods administration is um controls that regulation and then just controls that regulation and then just controls that regulation and then just leveraging opportunities to expand into leveraging opportunities to expand into leveraging opportunities to expand into other markets with compliance. you're other markets with compliance. you're other markets with compliance. you're going to have to take a look at some of going to have to take a look at some of going to have to take a look at some of these documents, but as we said, there's these documents, but as we said, there's these documents, but as we said, there's a lot of overlap um because the main a lot of overlap um because the main a lot of overlap um because the main goal is patient safety at the end of the goal is patient safety at the end of the goal is patient safety at the end of the day. And then just some recommendations. day. And then just some recommendations. day. And then just some recommendations. So, what manufacturers are applicable is So, what manufacturers are applicable is So, what manufacturers are applicable is going to be any of those that are going to be any of those that are going to be any of those that are located or they distribute within the located or they distribute within the located or they distribute within the region um that apply to specific region um that apply to specific region um that apply to specific regulations. So, if you're a regulations. So, if you're a regulations. So, if you're a manufacturer within the United States, manufacturer within the United States, manufacturer within the United States, you're going to have to apply um US you're going to have to apply um US you're going to have to apply um US regulations. If you're marketing towards regulations. If you're marketing towards regulations. If you're marketing towards European, you're also going to have to European, you're also going to have to European, you're also going to have to um make sure you're compliant with those um make sure you're compliant with those um make sure you're compliant with those regulations, which means you'll have regulations, which means you'll have regulations, which means you'll have some very pen devices, which is why some some very pen devices, which is why some some very pen devices, which is why some devices will be available in the EU but devices will be available in the EU but devices will be available in the EU but not available in the United States or not available in the United States or not available in the United States or vice versa. And then just some tips to vice versa. And then just some tips to vice versa. And then just some tips to um for the evolving landscape just um for the evolving landscape just um for the evolving landscape just ensure your AI system is transparent ensure your AI system is transparent ensure your AI system is transparent which means like what data is coming in which means like what data is coming in which means like what data is coming in um because we don't want any biases or um because we don't want any biases or um because we don't want any biases or ensuring that the device is cyber secure ensuring that the device is cyber secure ensuring that the device is cyber secure complying with existing laws and complying with existing laws and complying with existing laws and continuing to adapt to those changes as continuing to adapt to those changes as continuing to adapt to those changes as the United States continues to add the United States continues to add the United States continues to add regulations and recommendations. we're regulations and recommendations. we're regulations and recommendations. we're going to have to take those into account going to have to take those into account going to have to take those into account as well as what the AU AI does become as well as what the AU AI does become as well as what the AU AI does become applicable to a lot of different applicable to a lot of different applicable to a lot of different corporations um regularly audit the AI corporations um regularly audit the AI corporations um regularly audit the AI system for bias because if you're system for bias because if you're system for bias because if you're continuously putting in for example like continuously putting in for example like continuously putting in for example like a we're trying to diagnose a patient but a we're trying to diagnose a patient but a we're trying to diagnose a patient but we're continuously putting in other we're continuously putting in other we're continuously putting in other information into the AI model it information into the AI model it information into the AI model it could diagnosis and give that patient could diagnosis and give that patient could diagnosis and give that patient the wrong outcome. Um, we're going to the wrong outcome. Um, we're going to the wrong outcome. Um, we're going to have to keep the records of the data have to keep the records of the data have to keep the records of the data that's used to train the system to make that's used to train the system to make that's used to train the system to make sure it's kept up to date and then sure it's kept up to date and then sure it's kept up to date and then engage in industry collaboration AI into medical devices. collaboration AI into medical devices. collaboration AI into medical devices. It comes with a little bit of risk. It's It comes with a little bit of risk. It's It comes with a little bit of risk. It's it could be a double-edged sword where it could be a double-edged sword where it could be a double-edged sword where there is going to be a significant boost there is going to be a significant boost there is going to be a significant boost of productivity whatever that device is. of productivity whatever that device is. of productivity whatever that device is. But at that same cost and by But at that same cost and by But at that same cost and by implementing a disruptive technology and implementing a disruptive technology and implementing a disruptive technology and some of these things that um we're still some of these things that um we're still some of these things that um we're still very much on the the research curve of very much on the the research curve of very much on the the research curve of there could be um some unknown risks there could be um some unknown risks there could be um some unknown risks that we don't know yet. Uh there could that we don't know yet. Uh there could that we don't know yet. Uh there could also be supply chain risks for AI also be supply chain risks for AI also be supply chain risks for AI including training data. Maybe there's including training data. Maybe there's including training data. Maybe there's something in that training data that something in that training data that something in that training data that will cause it to have a bias that is not will cause it to have a bias that is not will cause it to have a bias that is not productive or the AI itself may productive or the AI itself may productive or the AI itself may hallucinate and produce hallucinate and produce hallucinate and produce false outputs. Each region will probably false outputs. Each region will probably false outputs. Each region will probably have their own regulations and that the have their own regulations and that the have their own regulations and that the manufacturers operate must comply with. manufacturers operate must comply with. manufacturers operate must comply with. So as Taylor was mentioning before, so So as Taylor was mentioning before, so So as Taylor was mentioning before, so it's not just the EU, it's also China, it's not just the EU, it's also China, it's not just the EU, it's also China, it's also the AP pack region of it's also the AP pack region of it's also the AP pack region of Singapore. It's global. So there may be Singapore. It's global. So there may be Singapore. It's global. So there may be this regulatory landscape that while this regulatory landscape that while this regulatory landscape that while certain global regions have their certain global regions have their certain global regions have their standardization baselines, those standardization baselines, those standardization baselines, those baselines may be different which is baselines may be different which is baselines may be different which is going to really make that going to really make that going to really make that capable or get even and navigating that capable or get even and navigating that capable or get even and navigating that maybe a challenge up on the horizon. And maybe a challenge up on the horizon. And maybe a challenge up on the horizon. And finally uh yeah kind of those unknown finally uh yeah kind of those unknown finally uh yeah kind of those unknown unknowns. So there are some known risks unknowns. So there are some known risks unknowns. So there are some known risks about AI right now, but there's also a about AI right now, but there's also a about AI right now, but there's also a lot of stuff that we don't know about AI lot of stuff that we don't know about AI lot of stuff that we don't know about AI right now and that could lead to risks right now and that could lead to risks right now and that could lead to risks that come up on the horizon. So it's that come up on the horizon. So it's that come up on the horizon. So it's definitely a cat and mouse game, but by definitely a cat and mouse game, but by definitely a cat and mouse game, but by proactively mitigating threats that are proactively mitigating threats that are proactively mitigating threats that are on the horizon, um, organizations can on the horizon, um, organizations can on the horizon, um, organizations can kind of smooth themselves into the kind of smooth themselves into the kind of smooth themselves into the transition instead of being reactionary transition instead of being reactionary transition instead of being reactionary when this needs to be compliant. Okay. when this needs to be compliant. Okay. when this needs to be compliant. Okay. So, if you're already preparing for So, if you're already preparing for So, if you're already preparing for compliance and you're preparing for the compliance and you're preparing for the compliance and you're preparing for the EUI act with some of this forward facing EUI act with some of this forward facing EUI act with some of this forward facing research, you'll already have some stuff research, you'll already have some stuff research, you'll already have some stuff in place before those deadlines are in place before those deadlines are in place before those deadlines are being set by those being set by those being set by those regulators. And with that being said, regulators. And with that being said, regulators. And with that being said, pass pass pass Q&amp;A. Anyone have any questions for us? I know. Um, you're mentioning medical advices Um, you're mentioning medical advices Um, you're mentioning medical advices specifically. I'm curious how this will specifically. I'm curious how this will specifically. I'm curious how this will also translate to things like insurance also translate to things like insurance also translate to things like insurance companies. Um we've seen a lot of do these clients insurance companies. So um these acts apply to a lot of stuff So um these acts apply to a lot of stuff So um these acts apply to a lot of stuff and I think they're going to apply to and I think they're going to apply to and I think they're going to apply to all AI systems but I think the best way all AI systems but I think the best way all AI systems but I think the best way to answer your question is that what to answer your question is that what to answer your question is that what categorization risk does it fall under? categorization risk does it fall under? categorization risk does it fall under? So I think medical devices just because So I think medical devices just because So I think medical devices just because they are accessing directly on protected they are accessing directly on protected they are accessing directly on protected health information. So some of those health information. So some of those health information. So some of those like biometric readings from the launch like biometric readings from the launch like biometric readings from the launch stuff like that they may have a higher stuff like that they may have a higher stuff like that they may have a higher regulatory requirement because they're regulatory requirement because they're regulatory requirement because they're classified as a high risk. If an classified as a high risk. If an classified as a high risk. If an insurance artificial intelligence system insurance artificial intelligence system insurance artificial intelligence system is also operating on protected health is also operating on protected health is also operating on protected health information so it's somehow looking at information so it's somehow looking at information so it's somehow looking at the procedures you received then I could the procedures you received then I could the procedures you received then I could see it also impacting being required to see it also impacting being required to see it also impacting being required to meet those high risk requirements. But meet those high risk requirements. But meet those high risk requirements. But if it's a system that's meant if it's a system that's meant if it's a system that's meant for maximizing efficiency in prices or for maximizing efficiency in prices or for maximizing efficiency in prices or generating protection plans, trying to generating protection plans, trying to generating protection plans, trying to figure out the best way to structure figure out the best way to structure figure out the best way to structure those that may not be as sensitive. So those that may not be as sensitive. So those that may not be as sensitive. So the these local regulations will likely the these local regulations will likely the these local regulations will likely apply to insurance applications, but at apply to insurance applications, but at apply to insurance applications, but at what level risk will they be? And I what level risk will they be? And I what level risk will they be? And I think there could be multiple systems in think there could be multiple systems in think there could be multiple systems in an insurance company that operate at an insurance company that operate at an insurance company that operate at different levels. So you could have one different levels. So you could have one different levels. So you could have one that would be a sensitive AI enabled that would be a sensitive AI enabled that would be a sensitive AI enabled system that that looks at the procedures system that that looks at the procedures system that that looks at the procedures that looks at um sensitive health that looks at um sensitive health that looks at um sensitive health information and you could have other information and you could have other information and you could have other ones that are just there to maximize ones that are just there to maximize ones that are just there to maximize efficiency that may not be sensitive and efficiency that may not be sensitive and efficiency that may not be sensitive and those breakdown specifically when it comes to breakdown specifically when it comes to breakdown specifically when it comes to data transferring Hey there. Um your example showed some Hey there. Um your example showed some Hey there. Um your example showed some uh different uh diagnostics for skin uh different uh diagnostics for skin uh different uh diagnostics for skin cancer. Um are there any industries that cancer. Um are there any industries that cancer. Um are there any industries that are really innovative about this or who are really innovative about this or who are really innovative about this or who are doing the security are doing the security are doing the security innovation or um what come to line with innovation or um what come to line with innovation or um what come to line with who's ahead of sure who's um ahead of it but your sure who's um ahead of it but your sure who's um ahead of it but your bigger companies are going to have a lot bigger companies are going to have a lot bigger companies are going to have a lot more range of what they're able to do more range of what they're able to do more range of what they're able to do the smaller companies obviously don't the smaller companies obviously don't the smaller companies obviously don't have those funings um I believe Philips have those funings um I believe Philips have those funings um I believe Philips Stevens would have these tools Stevens would have these tools Stevens would have these tools integrating already, but for integrating already, but for integrating already, but for specifically you're going to be looking specifically you're going to be looking specifically you're going to be looking at a lot of at a lot of at a lot of icons and systems. One last thing to those that are looking One last thing to those that are looking One last thing to those that are looking to see more information on what AI risks to see more information on what AI risks to see more information on what AI risks might be uh evolving, there's a couple might be uh evolving, there's a couple might be uh evolving, there's a couple resources you can check out. Uh the resources you can check out. Uh the resources you can check out. Uh the biter released the released the Atlas biter released the released the Atlas biter released the released the Atlas matrix, so it's similar to their attack. matrix, so it's similar to their attack. matrix, so it's similar to their attack. It's the it's the minor tactics for AI It's the it's the minor tactics for AI It's the it's the minor tactics for AI LLM enabled. So that's the tactics LLM enabled. So that's the tactics LLM enabled. So that's the tactics techniques and computers used by threat techniques and computers used by threat techniques and computers used by threat actors that also use AI. I'd also actors that also use AI. I'd also actors that also use AI. I'd also encourage you to re the LLM top 10 white encourage you to re the LLM top 10 white encourage you to re the LLM top 10 white paper series where it talks about the paper series where it talks about the paper series where it talks about the top 10 issues top 10 security issues top 10 issues top 10 security issues top 10 issues top 10 security issues with language models on the foundation. with language models on the foundation. with language models on the foundation. Thank you. Thank you. Thank you. All right, at this point it's probably a All right, at this point it's probably a All right, at this point it's probably a good time to give one more round of good time to give one more round of good time to give one more round of applause.